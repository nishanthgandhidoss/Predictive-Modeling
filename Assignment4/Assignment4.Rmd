---
title: "Assignment 4"
author: "Nishanth Gandhidoss"
date: "11/02/2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message = FALSE)
```


```{r installing necessary packages, include=FALSE}
# installing the packages
installNewPackage <- function(packageName) {
        if(packageName  %in% rownames(installed.packages()) == FALSE)
        {
                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)
        }
}

installNewPackage("kernlab")
installNewPackage("mlbench")
installNewPackage("caret")
installNewPackage("AppliedPredictiveModeling")

library(kernlab)
library(mlbench)
library(caret)
library(AppliedPredictiveModeling)
```


## Question 1 

For this question, we have given with some code that could generate sin curve with some noise. Lets use plot the data below and store that the data in a dataframe for future use. Also I have displayed the top five data points in the data

```{r Question1}
#  Code from the book
set.seed(1)
x <- runif(100, min = 2, max = 10)
y <- sin(x) + rnorm(length(x)) * .25
sinData <- data.frame(x = x, y = y)
plot(x, y)

## Create a grid of x values to use for prediction
dataGrid <- data.frame(x = seq(2, 10, length = 100))
head(dataGrid)
```


### Section (a)

As per the instructions, I am using a seed(1) in order to have reporduciability of my code. The following table shows the parameters values that I have used to fit the different models using radial basis functions.

```{r Question1(a), fig.height=30, fig.width=30}
# Setting the expand grid for the cost and epsilon parameters just like the last Home work
svm_parameter_grid <- expand.grid(costs = 2^c(-2, 0, 2, 8), epsilons = c(.01, .05, .1, .5))

# Show the grid used
svm_parameter_grid

# Intializing the empty result dataframe 
result_df <- data.frame(actual = double(), pred = double(), costs = factor(), epsilons = factor())

# Set the screen split
par(mfrow = c(4, 4))

# Looping over the combination of SVM parameter grid
for(index in 1:nrow(svm_parameter_grid)) {
    # Setting the seed
    set.seed(1)
    # Buliding the SVM
    rbf_model <- ksvm(x = x, y = y, data = sinData, kernel ="rbfdot", kpar = "automatic", C = svm_parameter_grid$costs[index],
                   epsilon = svm_parameter_grid$epsilons[index])

    # Make the prediction using the model generated
    prediction <- predict(rbf_model, newdata = dataGrid)
    plot(sinData$x, sinData$y, main = paste("Cost:", toString(svm_parameter_grid$costs[index]), "| Epsilon:", toString(svm_parameter_grid$epsilons[index])),
         xlab = "x", ylab = "y", cex = 2)
    points(dataGrid$x, prediction, type = "l", col = "red", lwd = 3)
    # Create the this iteration result data frame
    iter_results <- data.frame(actual = dataGrid$x, pred = prediction, costs = paste("Cost:", toString(svm_parameter_grid$costs[index])),
                                                             epsilons = paste("Epsilon:", toString(svm_parameter_grid$epsilons[index])))
    result_df <- rbind(result_df, iter_results)
}
```


Thus above we have the plot for various cost and epsilon values parameter fitted over the given curve. 

### Section (b)

Now lets add the sigma parameter as a hyperparamer. Each lines shows different sigma values with different colors.

```{r Question1(b), fig.height=30, fig.width=30}
# Setting seed for expand grid
set.seed(1)
# Setting the expand grid for the cost and epsilon parameters just like the last Home work
svm_parameter_grid <- expand.grid(costs = 2^c(-2, 0, 2, 8), epsilons = c(.01, .05, .1, .5))
sigma_values <- data.frame(sigma = c(0.2043268, 0.9977490, 47.74), color = c("red", "blue", "green"))

# Intializing the empty result dataframe 
result_df <- data.frame(actual = double(), pred = double(), costs = factor(), epsilons = factor())

# Set the screen split
par(mfrow = c(4, 4))

# Looping over the combination of SVM parameter grid
for(index in 1:nrow(svm_parameter_grid)) {
    plot(sinData$x, sinData$y, main = paste("Cost:", toString(svm_parameter_grid$costs[index]), "| Epsilon:", toString(svm_parameter_grid$epsilons[index])),
         xlab = "x", ylab = "y", col = "black", lwd = 2)
    
    for(i in 1:nrow(sigma_values)) {
        # Setting the seed
        set.seed(1)
        # Buliding the SVM
        rbf_model <- ksvm(x = x, y = y, data = sinData, kernel ="rbfdot", C = svm_parameter_grid$costs[index],
                       epsilon = svm_parameter_grid$epsilons[index], kpar = list(sigma = sigma_values$sigma[i]))
        # Make the prediction using the model generated
        prediction <- predict(rbf_model, newdata = dataGrid)
        points(dataGrid$x, prediction, type = "l", col = sigma_values$color[i], lwd = 3)
        # Create the this iteration result data frame
        iter_results <- data.frame(actual = dataGrid$x, pred = prediction, costs = paste("Cost:", toString(svm_parameter_grid$costs[index])),
                                                                 epsilons = paste("Epsilon:", toString(svm_parameter_grid$epsilons[index])))
        result_df <- rbind(result_df, iter_results)
    }
}

par(mai=c(0,0,0,0))
plot.new()
legend(x = "center", legend = sigma_values$sigma, col = sigma_values$color, lwd=4, cex=3, title = "Sigma Values", horiz = TRUE)
```

##### Cost

From the plot, we can see that increasing the cost will increase the model complexity. Thus model with low cost will have high bias while the model with high cost will have high variance.

##### Epsilon

From the above plot in the last column, we can see that increasing the epsilon value decreases that model complexity. The plot on the upper right corner has cost of 256 and epsilon of 0.91 with high sigma value is shown in red color. It looks overfitting the data but the plot in the bottom of the same columnn is having less complex model compared tmo the top one. We can say that increasing the epsilon will make the model smooth.

##### Sigma
The increase in the sigma values is overfiting the data. We can clearly see that the green line with low sigma value is having a very simple fit(high bias) compared to red line with very complex fit(high variance). 

## Question 2 

For this question we are going to work on the Friedman's dataset created by a simulation. mlbench package in R provides this data so that we can get it and use it. I going to load the train and test set of the data as it was directed in the book.

```{r Question2 load the data}
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)

## We convert the 'x' data from a matrix to a data frame
## One reason is that this will give the columns names.
trainingData$x <- data.frame(trainingData$x)

## Look at the data using
featurePlot(trainingData$x, trainingData$y)

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
```

Thus the above plot shows that we have succesfully loaded the train and test set.

#### KNN

As given in the book, lets first fit the data with a KNN model. For this I have used train() in the caret package with knn as method. Below we have complete information about the KNN model.

```{r Question2 KNN}
# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the KNN model
if(file.exists("models/knn_model_q2.rds")) {
    knn_model <- readRDS("models/knn_model_q2.rds")
} else {
    knn_model <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
    saveRDS(knn_model, "models/knn_model_q2.rds")
}

# Print the model
knn_model

# Plot the model
plot(knn_model)

# Predict the model
knn_pred <- predict(knn_model, newdata = testData$x)
```

#### Test set

```{r Question2 KNNi}
# Get the test Set performance metrics
postResample(pred = knn_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the best model has the k value of 15. That is the best R^2 is obtain when we are considering 15 nearest neighbours. The R^2 value obtained on the test set is 0.6786.

#### Neural Network

Lets bulid the model using neural network model using train() in caret package with method as nnet. I am setting the decay to be 0, 0.01, 0.1 and size varying from 1 to 10.

```{r Question2 Neural Network}
# Create the grid for the network
nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/nnet_model_q2.rds")) {
    nnet_model <- readRDS("models/nnet_model_q2.rds")
} else {
    nnet_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"),
                  linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(trainingData$x)+1) + 10 + 1, maxit=500)
    saveRDS(nnet_model, "models/nnet_model_q2.rds")
}

# Print the model
nnet_model

# Plot the model
plot(nnet_model)

# Predict the test set
nnet_pred <- predict(nnet_model, newdata = testData$x)
```

#### Test set

```{r Question2 Neural Networki} 
# Get the test Set performance metrics
postResample(pred = nnet_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the best model of neural network model is of size 1 and decay is 0.01. The R^2 value obtained on the test set is 0.7194.
    
#### Averaged Neural Network

Lets bulid the model using averaged neural network model using train() in caret package with method as avNNet. I am setting the decay to be 0, 0.001, 0.01, 0.1 and size varying from 1 to 10 with bag as False.

```{r Question2 Averaged Neural Network, warning=FALSE}
# Create the tune grid
tune_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = c(1:10), .bag = FALSE)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/avg_nnet_model_q2.rds")) {
    avg_nnet_model <- readRDS("models/avg_nnet_model_q2.rds")
} else {
    avg_nnet_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = tune_grid, method = "avNNet", preProc = c("center", "scale"),
                    linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1, maxit = 500)
    saveRDS(avg_nnet_model, "models/avg_nnet_model_q2.rds")
}

# Print the model
avg_nnet_model

# Plot the model
plot(avg_nnet_model)

# Make the prediction 
avg_nnet_pred <- predict(avg_nnet_model, newdata = testData$x)
```

#### Test set

```{r Question2 Averaged Neural Networki}
# Get the performance scores
postResample(pred = avg_nnet_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the best model of averaged neural network model has weight decay value of 0.1 and size as 3. The R^2 value obtained on the test set is 0.8339.
    
#### Mars Model with no Preprocessing

Lets bulid the model using mars model using train() in caret package with method as earth and preprocessing. I am setting the degree to be 1, 2, 3 and number of prune varying from 2 to 38.

```{r Question2 Mars}
# Create the tune grid
tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_q2.rds")) {
    mars_model <- readRDS("models/mars_model_q2.rds")
} else {
    mars_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_q2.rds")
}

# Print the model
mars_model

# Plot the model
plot(mars_model)

# Make the prediction 
mars_pred <- predict(mars_model, newdata = testData$x)
```

#### Test set

```{r Question2 Marsi}
# Get the performance scores
postResample(pred = mars_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 12 and degree = 2. The R^2 value obtained on the test set is 0.9335
    
#### MARS with spatial sign and correlated predictors removed 

Lets bulid the model using mars model using train() in caret package with method as earth and spatial signe. I am setting the degree to be 1, 2, 3 and number of prune varying from 2 to 38. I am checking the correlation between the predictors using .75 cut off and it seems that there is no highly correlated predictors. And upon buliding the model, we get

```{r Question2 Mars spatial}
# Remove the highly correlated value
highlyCorDescr <- findCorrelation(cor(trainingData$x), cutoff = .75)
trainingCorData <- trainingData$x[,-highlyCorDescr]

# Dimension of the predictors data
print(paste("No of highly correlated of predictors", dim(trainingCorData)[2]))

# Create the tune grid
tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_Spatial_q2.rds")) {
    mars_model1 <- readRDS("models/mars_model_Spatial_q2.rds")
} else {
    mars_model1 <- train(x = trainingData$x, y = trainingData$y, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_Spatial_q2.rds")
}

# Print the model
mars_model1

# Plot the model
plot(mars_model1)

# Make the prediction 
mars_pred <- predict(mars_model1, newdata = testData$x)
```

#### Test set

```{r Question2 Mars spatiali}
# Get the performance scores
postResample(pred = mars_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 12 and degree = 2. The R^2 value obtained on the test set is 0.8891. 

Thus by comparing to the above MARS model with no preprocessing R square value of 0.9335 this is quite low. Thus MARS model with no spatial transfrom is performing better on comparsion.

#### Support Vector Machine

Lets bulid the model using support vector machine using train() in caret package with svmRadial method which uses radial basis function. For svm, I set the tune length to be 14 as it tuneLength argument will use the default grid search of 20 cost values between 2^-2, 2^-1, . . . , 2^11. sigma is estimated analytically by default

```{r Question2 SVM}
# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/svm_model_q2.rds")) {
    svm_model <- readRDS("models/svm_model_q2.rds")
} else {
    svm_model <- svm_model <- train(x = trainingData$x, y = trainingData$y, tuneLength = 14, method = "svmRadial", preProc = c("center","scale"))
    saveRDS(svm_model, "models/svm_model_q2.rds")
}

# Print the model
svm_model

# Plot the model
plot(svm_model)

# Make the prediction 
svm_pred <- predict(svm_model, newdata = testData$x)
```

#### Test set

```{r Question2 SVMi}
# Get the performance scores
postResample(pred = svm_pred, obs = testData$y)
```

From the above plot and the tabulated result, we can clearly see that the final values used for the model were sigma = 0.06444911 and C = 2. The R^2 value obtained on the test set is 0.2052.

Thus from buliding up all the above models, I have the following table.

Model | Parameter |  Training RMSE | Training R Squared | Testing RMSE | Testing R Squared
----- | --------- | -------------- | ------------------ | ------------ | ----------------- | -----
    KNN  |   k as 15     |    3.2916    |   0.6124  |   3.1751     |   0.6786  |
    Neural Network  |   size as 1 & decay as 0.01     |    2.5028    |    0.7526  |   2.6433     |   0.7194  |
    Averaged Neural Network  |   size as 3 & decay as 0.1      |     2.3035    |   0.789  |   2.050   |   0.8339  |
    MARS with no Preprocessing  |   nprune as 12 & degree as 2    |    1.4588    |   0.9145  |   1.2803    |   0.9335  |
    MARS with spatial sign and correlated predictors removed  |   nprune as 12 & degree as 2    |     1.7686    |   0.8751  |   1.6543    |   0.8891  |
    Support Vector Machine  |   C as 2 & sigma as 0.06445   |    4.9139    |   0.1526  |   4.8632     |   0.2052   |

So from the table we can see that MARS model with no preprocessing gives best prediction as it has highest R^2 value 0.9335 on the test set. To answer the question regarding the predictor's importance on the MARS model I used varImp() and plotted below plot.

```{r MARS Var imp plot}
mars_mode_imp <- varImp(mars_model)
plot(mars_mode_imp, top = 10)
```

Now from the above plot we can clearly see that MARS model select only the informative predcitors that is x1 - x5 (in the order x2, x4, x1, x5, x3), as x6 to x10 has values as 0.

## Question 3

For this question we are going to work with the Tecator dataset and bulid various non linear regression models over it. Tecator dataset is a combination of absorp contains the 100 absorbance values for the 215 samples and endpoints contains the percent of moisture, fat, and protein in columns 1–3, respectively. Lets first load the data here.

```{r Question3}
# Load the data
data(tecator)
absorp_df <- as.data.frame(absorp)
endpoints_df <- as.data.frame(endpoints)
colnames(endpoints_df) = c("mositure", "fat","protein")

# Verify the data is loaded or not
head(absorp_df[1:5])
head(endpoints_df)
```

Thus we have above the data loaded properly. Lets get started with buliding the models.

Before we generate the model we need to split the data into training and testing samples. Given the sample size, we will retain the 80% of the samples to the training set and 20% of the sample in the testing set. The train set will be used to tune the models by splitting that into 10 fold for cross validation in order to have better model performance. For spliting the train set we will use Leave group out cross validation with 5 folds.

```{r Question3i, warning=FALSE}
# Setting the seed for reproduciablity
set.seed(1)

# Performing data spliting
cv_index <- createDataPartition(endpoints[, 2], p = 0.8, list = FALSE)
absorpTrain <- absorp_df[cv_index, ]
absorpTest <- absorp_df[-cv_index, ]
yTrain <- endpoints_df[cv_index, 2]
yTest <- endpoints_df[-cv_index, 2]

# Setting up the control parameter
ctrl <- trainControl(method = "LGOCV", repeats = 5)
```

#### KNN

Lets first fit the data with a KNN model. For this I have used train() in the caret package with knn as method. Below we have complete information about the KNN model.

```{r Question3 KNN}
# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the KNN model
if(file.exists("models/knn_model_q3.rds")) {
    knn_model <- readRDS("models/knn_model_q3.rds")
} else {
    knn_model <- train(x = absorpTrain, y = yTrain, method = "knn", preProc = c("center", "scale"), tuneLength = 10)
    saveRDS(knn_model, "models/knn_model_q3.rds")
}

# Print the model
knn_model

# Plot the model
plot(knn_model)

# Predict the model
knn_pred <- predict(knn_model, newdata = absorpTest)
```

#### Test set

```{r Question3 KNNi}
# Get the test Set performance metrics
postResample(pred = knn_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model has the k value of 5. That is the best R^2 is obtain when we are considering 5 nearest neighbours. The R^2 value obtained on the test set is 0.45.
    
#### Neural Network without PCA

Lets bulid the model using neural network model using train() in caret package with method as nnet. Here we are buliding it without PCA. I am setting the decay to be 0, 0.001, 0.01, 0.1 and size varying from 1 to 10.

```{r Question3 Neural Network, warning=FALSE}
# Create the grid for the network
nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/nnet_model_q3.rds")) {
    nnet_model <- readRDS("models/nnet_model_q3.rds")
} else {
    nnet_model <- train(x = absorpTrain, y = yTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"),
                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain)+1) + 10 + 1, maxit=500)
    saveRDS(nnet_model, "models/nnet_model_q3.rds")
}

# Print the model
nnet_model

# Plot the model
plot(nnet_model)

# Predict the test set
nnet_pred <- predict(nnet_model, newdata = absorpTest)
```


#### Test set

```{r Question3 Neural Networki} 
# Get the test Set performance metrics
postResample(pred = nnet_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of neural network model is of size 4 and decay is 0.01. The R^2 value obtained on the test set is 0.9987.
    
#### Neural Network with PCA

Let us bulid the model using PCA in the preprocessing argument of the train(). I am setting the decay to be 0, 0.001, 0.01, 0.1 and size varying from 1 to 10.

```{r Question3 Neural Network with pca, warning=FALSE}
# Create the grid for the network
nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/nnet_pca_model_q3.rds")) {
    nnet_pca_model <- readRDS("models/nnet_pca_model_q3.rds")
} else {
    nnet_pca_model <- train(x = absorpTrain, y = yTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale", "pca"),
                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain)+1) + 10 + 1, maxit=500)
    saveRDS(nnet_pca_model, "models/nnet_pca_model_q3.rds")
}

# Print the model
nnet_pca_model

# Plot the model
plot(nnet_pca_model)

# Predict the test set
nnet_pca_pred <- predict(nnet_pca_model, newdata = absorpTest)
```

#### Test set

```{r Question3 Neural Network with pcai} 
# Get the test Set performance metrics
postResample(pred = nnet_pca_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of neural network model is of size 2 and decay is 0.1. The R^2 value obtained on the test set is 0.2661. It is very low compared to neural network without pcs. Thus PCA is not helping much.

#### Averaged Neural Network

Lets bulid the model using averaged neural network model using train() in caret package with method as avNNet. I am setting the decay to be 0, 0.001, 0.01, 0.1 and size varying from 1 to 10 with bag as False.

```{r Question3 Averaged Neural Network, warning=FALSE}
# Create the tune grid
tune_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = 1:10, .bag = FALSE)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/avg_nnet_model_q3.rds")) {
    avg_nnet_model <- readRDS("models/avg_nnet_model_q3.rds")
} else {
    avg_nnet_model <- train(x = absorpTrain, y = yTrain, tuneGrid = tune_grid, method = "avNNet", preProc = c("center", "scale"),
                            linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain) + 1) + 10 + 1, maxit = 500)
    saveRDS(avg_nnet_model, "models/avg_nnet_model_q3.rds")
}

# Print the model
avg_nnet_model

# Plot the model
plot(avg_nnet_model)

# Make the prediction 
avg_nnet_pred <- predict(avg_nnet_model, newdata = absorpTest)
```

#### Test set

```{r Question3 Averaged Neural Networki}
# Get the performance scores
postResample(pred = avg_nnet_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of averaged neural network model has weight decay value of 0.1 and size as 4. The R^2 value obtained on the test set is 0.9989.
    
#### Mars Model with no preprocessing

Lets bulid the model using mars model with train() in caret package with method as earth. I am setting the degree to be 1, 2, 3 and number of prune varying from 2 to 38.

```{r Question3 Mars}
# Create the tune grid
tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_q3.rds")) {
    mars_model <- readRDS("models/mars_model_q3.rds")
} else {
    mars_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_q3.rds")
}

# Print the model
mars_model

# Plot the model
plot(mars_model)

# Make the prediction 
mars_pred <- predict(mars_model, newdata = absorpTest)
```

#### Test set

```{r Question3 Marsi}
# Get the performance scores
postResample(pred = mars_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 24 and degree = 2. The R^2 value obtained on the test set is 0.985.

#### Mars Model with spatial sign removing correlated predictors

When I tried removing the correlated predictors for MARS model even with threshold of .999 I was getting only one predictors ie) first predictors. So I used that predictor to create the model

```{r Question3 Mars spatial}
# Remove the highly correlated value
highlyCorDescr <- findCorrelation(absorp_df, cutoff = 0.999)
absorp_cor_df <- absorp_df[,-highlyCorDescr]

# Creating the test and train after removing correlated predictors
absorpCorTrain <- as.data.frame(absorp_cor_df[cv_index])
absorpCorTest <- as.data.frame(absorp_cor_df[-cv_index])

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_Spatial_q3.rds")) {
    mars_model <- readRDS("models/mars_model_Spatial_q3.rds")
} else {
    mars_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_Spatial_q3.rds")
}

# Print the model
mars_model

# Plot the model
plot(mars_model)

# Make the prediction 
mars_pred <- predict(mars_model, newdata = absorpTest)
```

#### Test set

```{r Question3 Mars spatiali}
# Get the performance scores
postResample(pred = mars_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 13 and degree = 1. The R^2 value obtained on the test set is 0.8156.


#### Support Vector Machine

Lets bulid the model using support vector machine using train() in caret package with svmRadial method which uses radial basis function. For svm, I set the tune length to be 14 as it tuneLength argument will use the default grid search of 20 cost values between 2^-2, 2^-1, . . . , 2^11. sigma is estimated analytically by default

```{r Question3 SVM}
# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/svm_model_q3.rds")) {
    svm_model <- readRDS("models/svm_model_q3.rds")
} else {
    svm_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, tuneLength = 14, method = "svmRadial", preProc = c("center","scale"))
    saveRDS(svm_model, "models/svm_model_q3.rds")
}

# Print the model
svm_model

# Plot the model
plot(svm_model)

# Make the prediction 
svm_pred <- predict(svm_model, newdata = absorpTest)
```

#### Test set

```{r Question3 SVMi}
# Get the performance scores
postResample(pred = svm_pred, obs = yTest)
```

From the above plot and the tabulated result, we can clearly see that the final values used for the model were sigma = 0.06775675 and C = 8. The R^2 value obtained on the test set is 0.7054.
    
Thus from buliding up all the above models, I have the following table.

Model | Parameter |  Training RMSE | Training R Squared | Testing RMSE | Testing R Squared
----- | --------- | -------------- | ------------------ | ------------ | ----------------- | 
    KNN  |   k as 5     |    9.1336    |   0.5222  |   8.3322     |   0.45  |
    Neural Network without PCA |   size as 4 & decay as 0.01     |    0.8409    |    0.9954  |   9.5493     |   0.9987  |
    Neural Network with PCA |   size as 2 & decay as 0.01     |    12.0004    |    0.2054  |    9.5493     |  0.2661  |
    Averaged Neural Network  |   size as 3 & decay as 0.01      |    0.6935    |   0.9973  |   0.3869     |   0.9989  |
    Mars Model with no preprocessing  |   nprune as 24 & degree as 2    |    2.7309    |   0.9601  |   1.3921    |   0.985  |
    Mars Model with spatial sign removing correlated predictors  |   nprune = 13 & degree = 1    |    6.1338    |   0.8042  |   5.3556    |   0.8156  |
    Support Vector Machine  |   C as 8 & sigma as 0.06775675   |    7.5854     |   0.6885  |    6.0048    |   0.7054   |

So from the table we can see that the average neural network model gives best prediction as it has highest R^2 (0.9989) value on the test set. Coming back to the question of neural network sensitive to highly correlated predictors, we can clearly see that neurtal network without PCA (Test R square:- 0.9987) is performing better thsn the one with PCA (Test R square:- 0.2661). 

Thus using proprocessing using PCA on neural does not helps.

## Question 4 

We will first load the dataset here in order to answer the further question. To ensure I have the data I checked the data dimension and printed out a small subset of the data.

```{r Question4}
data(permeability)
fingerprints_df <- as.data.frame(fingerprints)
head(fingerprints_df[, 1:5])
permeability[1:5]
```

Thus we see that both the predictors and target values are loaded properly. The fingerprints contains the 1107 binary molecular predictors for the 165 compounds, while permeability contains permeability response is the responding variable. It is been told in Exercise 6.2 that the fingerprint predictors are having sparse data. In order to handle this we are going to apply near zero variance to handle this using caret package.

```{r Question4i}
fingerprints_filtered_df <- fingerprints_df[, -nearZeroVar(fingerprints_df)]
print(paste("Number of predictors left out for modeling is", dim(fingerprints_filtered_df)[2]))
```

Thus after removing the near zero variance from the model we have 388 predictors left our in the data. We will use this predictors to bulid the models.

To split the data into a training and a test set, I have 80% of the data put into the train set and left out data samples in the train set. Then using the splitted train set, I will use Leave group out cross validation because the size of the data points is considerably greater than the size of the predictors. Lets bulid the models.

```{r Question4ii, warning=FALSE}
# Setting the seed for reproduciablity
set.seed(1)

# Performing data spliting
cv_index <- createDataPartition(permeability, p = 0.8, list = FALSE)
fingerprintsTrain <- fingerprints_filtered_df[cv_index,]
fingerprintsTest <- fingerprints_filtered_df[-cv_index,]
permeabilityTrain <- permeability[cv_index]
permeabilityTest <- permeability[-cv_index]

# Setting up the control parameter
ctrl <- trainControl(method = "LGOCV")
```

#### KNN

Lets first fit the data with a KNN model. For this I have used train() in the caret package with knn as method. Below we have complete information about the KNN model.

```{r Question4 KNN}
# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the KNN model
if(file.exists("models/knn_model_q4.rds")) {
    knn_model <- readRDS("models/knn_model_q4.rds")
} else {
    knn_model <- train(x = fingerprintsTrain, y = permeabilityTrain, method = "knn", preProc = c("center", "scale"), trControl = ctrl, tuneLength = 10)
    saveRDS(knn_model, "models/knn_model_q4.rds")
}

# Print the model
knn_model

# Plot the model
plot(knn_model)

# Predict the model
knn_pred <- predict(knn_model, newdata = fingerprintsTest)
```

#### Test set

```{r Question4 KNNi}
# Get the test Set performance metrics
postResample(pred = knn_pred, obs = permeabilityTest)
```

From the above plot and the tabulated result, we can clearly see that the best model has the k value of 9. That is the best R^2 is obtain when we are considering 9 nearest neighbours. The R^2 value obtained on the test set is 0.3786.

#### Neural Network

Lets bulid the model using neural network model using train() in caret package with method as nnet. I am setting the decay to be 0, 0.01, 0.1 and size varying from 1 to 10.

```{r Question4 Neural Network}
# Create the grid for the network
nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

# Set the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/nnet_model_q4.rds")) {
    nnet_model <- readRDS("models/nnet_model_q4.rds")
} else {
    nnet_model <- train(x = fingerprintsTrain, y = permeabilityTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"), trControl = ctrl, 
                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(fingerprintsTrain)+1) + 10 + 1, maxit=500)
    saveRDS(nnet_model, "models/nnet_model_q4.rds")
}

# Print the model
nnet_model

# Plot the model
plot(nnet_model)

# Predict the test set
nnet_pred <- predict(nnet_model, newdata = fingerprintsTest)
```


#### Test set

```{r Question4 Neural Networki} 
# Get the test Set performance metrics
postResample(pred = nnet_pred, obs = permeabilityTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of neural network model is of size 5 and decay is 0. The R^2 value obtained on the test set is 0.2915.

#### Mars Model with no preprocessing

Lets bulid the model using mars model using train() in caret package with method as earth. I am setting the degree to be 1, 2, 3 and number of prune varying from 2 to 38. I tried applying spatial sign for preprocess which gave me worst prediction on the test set with 0.23 R square value. Thus I used center and scaling as preprocess step for the Mars model and below the details.

```{r Question4 Mars, warning=FALSE}
# Create the tune grid
tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_q4.rds")) {
    mars_model <- readRDS("models/mars_model_q4.rds")
} else {
    mars_model <- train(x = fingerprintsTrain, y = permeabilityTrain, trControl = ctrl, tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_q4.rds")
}

# Print the model
mars_model

# Plot the model
plot(mars_model)

# Make the prediction 
mars_pred <- predict(mars_model, newdata = fingerprintsTest)
```

#### Test set

```{r Question4 Marsi}
# Get the performance scores
postResample(pred = mars_pred, obs = permeabilityTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 2 and degree = 1. The R^2 value obtained on the test set is 0.5469.
    
#### Mars Model removing correlation

Lets bulid the model using mars model using train() in caret package with method as earth. With a small change here that we will remove the correlated predictors out and then train the model. I am setting the degree to be 1, 2, 3 and number of prune varying from 2 to 38.

```{r Question4 Mars Cor, warning=FALSE}
# Remove the highly correlated value
highlyCorDescr <- findCorrelation(fingerprints_filtered_df, cutoff = .75)
fingerprints_filtered_cor_df <- fingerprints_filtered_df[,-highlyCorDescr]

# Split the data
fingerprintsCorTrain <- fingerprints_filtered_cor_df[cv_index,]
fingerprintsCorTest <- fingerprints_filtered_cor_df[-cv_index,]

# Create the tune grid
tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/mars_model_cor_q4.rds")) {
    mars_model <- readRDS("models/mars_model_cor_q4.rds")
} else {
    mars_model <- train(x = fingerprintsCorTrain, y = permeabilityTrain, trControl = ctrl, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")
    saveRDS(mars_model, "models/mars_model_cor_q4.rds")
}

# Print the model
mars_model

# Plot the model
plot(mars_model)

# Make the prediction 
mars_pred <- predict(mars_model, newdata = fingerprintsCorTest)
```

#### Test set

```{r Question4 Mars Cor i}
# Get the performance scores
postResample(pred = mars_pred, obs = permeabilityTest)
```

From the above plot and the tabulated result, we can clearly see that the best model of mars model has nprune = 7 and degree = 1. The R^2 value obtained on the test set is 0.1372.
    
#### Support Vector Machine

Lets bulid the model using support v ector machine using train() in caret package with svmRadial method which uses radial basis function. For svm, I set the tune length to be 14 as it tuneLength argument will use the default grid search of 20 cost values between 2^-2, 2^-1, . . . , 2^11. sigma is estimated analytically by default

```{r Question4 SVM}
# Setting the seed
set.seed(1)

# Check the file exists and load to variables
# else bulid and store the model
if(file.exists("models/svm_model_q4.rds")) {
    svm_model <- readRDS("models/svm_model_q4.rds")
} else {
    svm_model <- train(x = fingerprintsTrain, y = permeabilityTrain, tuneLength = 14, method = "svmRadial", 
                       trControl = ctrl, preProc = c("center","scale"))
    saveRDS(svm_model, "models/svm_model_q4.rds")
}

# Print the model
svm_model

# Plot the model
plot(svm_model)

# Make the prediction 
svm_pred <- predict(svm_model, newdata = fingerprintsTest)
```

#### Test set

```{r Question4 SVMi}
# Get the performance scores
postResample(pred = svm_pred, obs = permeabilityTest)
```

From the above plot and the tabulated result, we can clearly see that the final values used for the model were sigma = 0.00233 and C = 2. The R^2 value obtained on the test set is 0.3989.
    
Thus from buliding up all the above models, I have the following table.

Model | Parameter |  Training RMSE | Training R Squared | Testing RMSE | Testing R Squared
----- | --------- | -------------- | ------------------ | ------------ | ----------------- | 
    KNN  |   k as 9     |    12.1357    |   0.4234  |   11.5585     |   0.3786  |
    Neural Network  |   size as 5 & decay as 0     |    12.8981    |    0.4091  |   14.2703     |   0.2915  |
    MARS with no preprocessing |   nprune as 2 & degree as 2    |    12.4215    |    0.4099   |   9.8693     |   0.5469  |
    MARS Removing correlation  |   nprune as 7 & degree as 1    |    14.6325     |    0.1884   |   14.1888       |   0.1372  |
    Support Vector Machine  |   C as 2 & sigma as 0.00233   |    15.4733   |   0.0200  |   11.7033     |   0.3989   |

### Section (a)

From the above table we can clearly see that MARS model is performing both on optimal resampling and test set with comparatively high R square value as 0.4099 on training and 0.5469 on the testing set.

### Section (b)

Yes, MARS model performs better than the optimal linear model which is a Partial Least Square model with 7 components (Test set R square: 0.4457). MARS model has the test set R square value of 0.5469. This shows that the underlying relationship between the predictors and responding variable is non linear.

### Section (c)

No, I won't recommend the model because the best model MARS itself has a test set R square value of 0.5469 which quite on the lower end.

*** End of Solution ***

### Appendix - Coding

\# installing the packages

installNewPackage <- function(packageName) {

        if(packageName  %in% rownames(installed.packages()) == FALSE)

        {

                install.packages(packageName, repos = "http://cran.us.r-project.org", dependencies=TRUE)

        }

}

installNewPackage("kernlab")

installNewPackage("mlbench")

installNewPackage("caret")

installNewPackage("AppliedPredictiveModeling")

library(kernlab)

library(mlbench)

library(caret)

library(AppliedPredictiveModeling)

## Question 1 

\#  Code from the book

set.seed(1)

x <- runif(100, min = 2, max = 10)

y <- sin(x) + rnorm(length(x)) * .25

sinData <- data.frame(x = x, y = y)

plot(x, y)

\#\# Create a grid of x values to use for prediction

dataGrid <- data.frame(x = seq(2, 10, length = 100))

head(dataGrid)

### Section (a)

\# Setting the expand grid for the cost and epsilon parameters just like the last Home work

svm_parameter_grid <- expand.grid(costs = 2^c(-2, 0, 2, 8), epsilons = c(.01, .05, .1, .5))

\# Show the grid used

svm_parameter_grid

\# Intializing the empty result dataframe 

result_df <- data.frame(actual = double(), pred = double(), costs = factor(), epsilons = factor())

\# Set the screen split

par(mfrow = c(4, 4))

\# Looping over the combination of SVM parameter grid

for(index in 1:nrow(svm_parameter_grid)) {

    \# Setting the seed

    set.seed(1)

    \# Buliding the SVM

    rbf_model <- ksvm(x = x, y = y, data = sinData, kernel ="rbfdot", kpar = "automatic", C = svm_parameter_grid$costs[index],

                   epsilon = svm_parameter_grid$epsilons[index])


    \# Make the prediction using the model generated

    prediction <- predict(rbf_model, newdata = dataGrid)

    plot(sinData$x, sinData$y, main = paste("Cost:", toString(svm_parameter_grid$costs[index]), "| Epsilon:", toString(svm_parameter_grid$epsilons[index])),

         xlab = "x", ylab = "y", cex = 2)

    points(dataGrid$x, prediction, type = "l", col = "red", lwd = 3)

    \# Create the this iteration result data frame

    iter_results <- data.frame(actual = dataGrid$x, pred = prediction, costs = paste("Cost:", toString(svm_parameter_grid$costs[index])),

                                                             epsilons = paste("Epsilon:", toString(svm_parameter_grid$epsilons[index])))

    result_df <- rbind(result_df, iter_results)

}

### Section (b)

\# Setting seed for expand grid

set.seed(1)

\# Setting the expand grid for the cost and epsilon parameters just like the last Home work

svm_parameter_grid <- expand.grid(costs = 2^c(-2, 0, 2, 8), epsilons = c(.01, .05, .1, .5))

sigma_values <- data.frame(sigma = c(0.2043268, 0.9977490, 47.74), color = c("red", "blue", "green"))

\# Intializing the empty result dataframe 

result_df <- data.frame(actual = double(), pred = double(), costs = factor(), epsilons = factor())

\# Set the screen split

par(mfrow = c(4, 4))

\# Looping over the combination of SVM parameter grid

for(index in 1:nrow(svm_parameter_grid)) {

    plot(sinData$x, sinData$y, main = paste("Cost:", toString(svm_parameter_grid$costs[index]), "| Epsilon:", toString(svm_parameter_grid$epsilons[index])),

         xlab = "x", ylab = "y", col = "black", lwd = 2)

    for(i in 1:nrow(sigma_values)) {

        \# Setting the seed

        set.seed(1)

        \# Buliding the SVM

        rbf_model <- ksvm(x = x, y = y, data = sinData, kernel ="rbfdot", C = svm_parameter_grid$costs[index],

                       epsilon = svm_parameter_grid$epsilons[index], kpar = list(sigma = sigma_values$sigma[i]))

        \# Make the prediction using the model generated

        prediction <- predict(rbf_model, newdata = dataGrid)

        points(dataGrid$x, prediction, type = "l", col = sigma_values$color[i], lwd = 3)

        \# Create the this iteration result data frame

        iter_results <- data.frame(actual = dataGrid$x, pred = prediction, costs = paste("Cost:", toString(svm_parameter_grid$costs[index])),

                                                                 epsilons = paste("Epsilon:", toString(svm_parameter_grid$epsilons[index])))

        result_df <- rbind(result_df, iter_results)

    }

}

par(mai=c(0,0,0,0))

plot.new()

legend(x = "center", legend = sigma_values$sigma, col = sigma_values$color, lwd=4, cex=3, title = "Sigma Values", horiz = TRUE)

## Question 2 

set.seed(200)

trainingData <- mlbench.friedman1(200, sd = 1)

\# We convert the 'x' data from a matrix to a data frame

\# One reason is that this will give the columns names.

trainingData$x <- data.frame(trainingData$x)

\# Look at the data using

featurePlot(trainingData$x, trainingData$y)

\# This creates a list with a vector 'y' and a matrix

\# of predictors 'x'. Also simulate a large test set to

\# estimate the true error rate with good precision:

testData <- mlbench.friedman1(5000, sd = 1)

testData$x <- data.frame(testData$x)

#### KNN

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the KNN model

if(file.exists("models/knn_model_q2.rds")) {

    knn_model <- readRDS("models/knn_model_q2.rds")

} else {

    knn_model <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10)

    saveRDS(knn_model, "models/knn_model_q2.rds")

}

\# Print the model

knn_model

\# Plot the model

plot(knn_model)

\# Predict the model

knn_pred <- predict(knn_model, newdata = testData$x)

\# Get the test Set performance metrics

postResample(pred = knn_pred, obs = testData$y)

#### Neural Network

\# Create the grid for the network

nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/nnet_model_q2.rds")) {

    nnet_model <- readRDS("models/nnet_model_q2.rds")

} else {

    nnet_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"),

                  linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(trainingData$x)+1) + 10 + 1, maxit=500)

    saveRDS(nnet_model, "models/nnet_model_q2.rds")

}

\# Print the model

nnet_model

\# Plot the model

plot(nnet_model)

\# Predict the test set

nnet_pred <- predict(nnet_model, newdata = testData$x)

\# Get the test Set performance metrics

postResample(pred = nnet_pred, obs = testData$y)

#### Averaged Neural Network

\# Create the tune grid

tune_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = c(1:10), .bag = FALSE)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/avg_nnet_model_q2.rds")) {

    avg_nnet_model <- readRDS("models/avg_nnet_model_q2.rds")

} else {

    avg_nnet_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = tune_grid, method = "avNNet", preProc = c("center", "scale"),

                    linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(trainingData$x) + 1) + 10 + 1, maxit = 500)

    saveRDS(avg_nnet_model, "models/avg_nnet_model_q2.rds")
}

\# Print the model

avg_nnet_model

\# Plot the model

plot(avg_nnet_model)

\# Make the prediction 

avg_nnet_pred <- predict(avg_nnet_model, newdata = testData$x)

\# Get the performance scores

postResample(pred = avg_nnet_pred, obs = testData$y)

#### Mars Model with no Preprocessing

\# Create the tune grid

tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_q2.rds")) {

    mars_model <- readRDS("models/mars_model_q2.rds")

} else {

    mars_model <- train(x = trainingData$x, y = trainingData$y, tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_q2.rds")

}

\# Print the model

mars_model

\# Plot the model

plot(mars_model)

\# Make the prediction 

mars_pred <- predict(mars_model, newdata = testData$x)

\# Get the performance scores

postResample(pred = mars_pred, obs = testData$y)

#### MARS with spatial sign and correlated predictors removed 

\# Remove the highly correlated value

highlyCorDescr <- findCorrelation(cor(trainingData$x), cutoff = .75)

trainingCorData <- trainingData$x[,-highlyCorDescr]

\# Dimension of the predictors data

print(paste("No of highly correlated of predictors", dim(trainingCorData)[2]))

\# Create the tune grid

tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_Spatial_q2.rds")) {

    mars_model1 <- readRDS("models/mars_model_Spatial_q2.rds")

} else {

    mars_model1 <- train(x = trainingData$x, y = trainingData$y, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_Spatial_q2.rds")

}

\# Print the model

mars_model1

\# Plot the model

plot(mars_model1)

\# Make the prediction 

mars_pred <- predict(mars_model1, newdata = testData$x)

\# Get the performance scores

postResample(pred = mars_pred, obs = testData$y)

#### Support Vector Machine

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/svm_model_q2.rds")) {

    svm_model <- readRDS("models/svm_model_q2.rds")

} else {

    svm_model <- svm_model <- train(x = trainingData$x, y = trainingData$y, tuneLength = 14, method = "svmRadial", preProc = c("center","scale"))

    saveRDS(svm_model, "models/svm_model_q2.rds")

}

\# Print the model

svm_model

\# Plot the model

plot(svm_model)

\# Make the prediction 

svm_pred <- predict(svm_model, newdata = testData$x)

\# Get the performance scores

postResample(pred = svm_pred, obs = testData$y)

mars_mode_imp <- varImp(mars_model)

plot(mars_mode_imp, top = 10)

## Question 3

\# Load the data

data(tecator)

absorp_df <- as.data.frame(absorp)

endpoints_df <- as.data.frame(endpoints)

colnames(endpoints_df) = c("mositure", "fat","protein")

\# Verify the data is loaded or not

head(absorp_df[1:5])

head(endpoints_df)

\# Setting the seed for reproduciablity

set.seed(1)

\# Performing data spliting

cv_index <- createDataPartition(endpoints[, 2], p = 0.8, list = FALSE)

absorpTrain <- absorp_df[cv_index, ]

absorpTest <- absorp_df[-cv_index, ]

yTrain <- endpoints_df[cv_index, 2]

yTest <- endpoints_df[-cv_index, 2]

\# Setting up the control parameter

ctrl <- trainControl(method = "LGOCV", repeats = 5)

#### KNN

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the KNN model

if(file.exists("models/knn_model_q3.rds")) {

    knn_model <- readRDS("models/knn_model_q3.rds")

} else {

    knn_model <- train(x = absorpTrain, y = yTrain, method = "knn", preProc = c("center", "scale"), tuneLength = 10)

    saveRDS(knn_model, "models/knn_model_q3.rds")

}

\# Print the model

knn_model

\# Plot the model

plot(knn_model)

\# Predict the model

knn_pred <- predict(knn_model, newdata = absorpTest)

\# Get the test Set performance metrics

postResample(pred = knn_pred, obs = yTest)

#### Neural Network without PCA

\# Create the grid for the network

nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/nnet_model_q3.rds")) {

    nnet_model <- readRDS("models/nnet_model_q3.rds")

} else {

    nnet_model <- train(x = absorpTrain, y = yTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"),

                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain)+1) + 10 + 1, maxit=500)

    saveRDS(nnet_model, "models/nnet_model_q3.rds")

}

\# Print the model

nnet_model

\# Plot the model

plot(nnet_model)

\# Predict the test set

nnet_pred <- predict(nnet_model, newdata = absorpTest)

\# Get the test Set performance metrics

postResample(pred = nnet_pred, obs = yTest)

#### Neural Network with PCA

\# Create the grid for the network

nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/nnet_pca_model_q3.rds")) {

    nnet_pca_model <- readRDS("models/nnet_pca_model_q3.rds")

} else {

    nnet_pca_model <- train(x = absorpTrain, y = yTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale", "pca"),

                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain)+1) + 10 + 1, maxit=500)

    saveRDS(nnet_pca_model, "models/nnet_pca_model_q3.rds")

}

\# Print the model

nnet_pca_model

\# Plot the model

plot(nnet_pca_model)

\# Predict the test set

nnet_pca_pred <- predict(nnet_pca_model, newdata = absorpTest)

\# Get the test Set performance metrics

postResample(pred = nnet_pca_pred, obs = yTest)

#### Averaged Neural Network

\# Create the tune grid

tune_grid <- expand.grid(.decay = c(0, 0.01, .1), .size = 1:10, .bag = FALSE)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/avg_nnet_model_q3.rds")) {

    avg_nnet_model <- readRDS("models/avg_nnet_model_q3.rds")

} else {

    avg_nnet_model <- train(x = absorpTrain, y = yTrain, tuneGrid = tune_grid, method = "avNNet", preProc = c("center", "scale"),

                            linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(absorpTrain) + 1) + 10 + 1, maxit = 500)

    saveRDS(avg_nnet_model, "models/avg_nnet_model_q3.rds")

}

\# Print the model

avg_nnet_model

\# Plot the model

plot(avg_nnet_model)

\# Make the prediction 

avg_nnet_pred <- predict(avg_nnet_model, newdata = absorpTest)

\# Get the performance scores

postResample(pred = avg_nnet_pred, obs = yTest)
    
#### Mars Model with no preprocessing

\# Create the tune grid

tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_q3.rds")) {

    mars_model <- readRDS("models/mars_model_q3.rds")

} else {

    mars_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_q3.rds")

}

\# Print the model

mars_model

\# Plot the model

plot(mars_model)

\# Make the prediction 

mars_pred <- predict(mars_model, newdata = absorpTest)

\# Get the performance scores

postResample(pred = mars_pred, obs = yTest)

#### Mars Model with spatial sign removing correlated predictors

\# Remove the highly correlated value

highlyCorDescr <- findCorrelation(absorp_df, cutoff = 0.999)

absorp_cor_df <- absorp_df[,-highlyCorDescr]

\# Creating the test and train after removing correlated predictors

absorpCorTrain <- as.data.frame(absorp_cor_df[cv_index])

absorpCorTest <- as.data.frame(absorp_cor_df[-cv_index])

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_Spatial_q3.rds")) {

    mars_model <- readRDS("models/mars_model_Spatial_q3.rds")

} else {

    mars_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_Spatial_q3.rds")

}

\# Print the model

mars_model

\# Plot the model

plot(mars_model)

\# Make the prediction 

mars_pred <- predict(mars_model, newdata = absorpTest)

\# Get the performance scores

postResample(pred = mars_pred, obs = yTest)

#### Support Vector Machine

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/svm_model_q3.rds")) {

    svm_model <- readRDS("models/svm_model_q3.rds")

} else {

    svm_model <- train(x = absorpTrain, y = yTrain, trControl = ctrl, tuneLength = 14, method = "svmRadial", preProc = c("center","scale"))

    saveRDS(svm_model, "models/svm_model_q3.rds")

}

\# Print the model

svm_model

\# Plot the model

plot(svm_model)

\# Make the prediction 

svm_pred <- predict(svm_model, newdata = absorpTest)

\# Get the performance scores

postResample(pred = svm_pred, obs = yTest)

## Question 4 

data(permeability)

fingerprints_df <- as.data.frame(fingerprints)

head(fingerprints_df[, 1:5])

permeability[1:5]

fingerprints_filtered_df <- fingerprints_df[, -nearZeroVar(fingerprints_df)]

print(paste("Number of predictors left out for modeling is", dim(fingerprints_filtered_df)[2]))

\# Setting the seed for reproduciablity

set.seed(1)

\# Performing data spliting

cv_index <- createDataPartition(permeability, p = 0.8, list = FALSE)

fingerprintsTrain <- fingerprints_filtered_df[cv_index,]

fingerprintsTest <- fingerprints_filtered_df[-cv_index,]

permeabilityTrain <- permeability[cv_index]

permeabilityTest <- permeability[-cv_index]

\# Setting up the control parameter

ctrl <- trainControl(method = "LGOCV")

#### KNN

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the KNN model

if(file.exists("models/knn_model_q4.rds")) {

    knn_model <- readRDS("models/knn_model_q4.rds")

} else {

    knn_model <- train(x = fingerprintsTrain, y = permeabilityTrain, method = "knn", preProc = c("center", "scale"), trControl = ctrl, tuneLength = 10)

    saveRDS(knn_model, "models/knn_model_q4.rds")

}

\# Print the model

knn_model

\# Plot the model

plot(knn_model)

\# Predict the model

knn_pred <- predict(knn_model, newdata = fingerprintsTest)

\# Get the test Set performance metrics

postResample(pred = knn_pred, obs = permeabilityTest)

#### Neural Network

\# Create the grid for the network

nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1), .size = 1:10)

\# Set the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/nnet_model_q4.rds")) {

    nnet_model <- readRDS("models/nnet_model_q4.rds")

} else {

    nnet_model <- train(x = fingerprintsTrain, y = permeabilityTrain, tuneGrid = nn_grid, method = "nnet", preProc = c("center", "scale"), trControl = ctrl, 

                        linout = TRUE, trace = FALSE, MaxNWts = 10 * (ncol(fingerprintsTrain)+1) + 10 + 1, maxit=500)

    saveRDS(nnet_model, "models/nnet_model_q4.rds")

}

\# Print the model

nnet_model

\# Plot the model

plot(nnet_model)

\# Predict the test set

nnet_pred <- predict(nnet_model, newdata = fingerprintsTest)

\# Get the test Set performance metrics

postResample(pred = nnet_pred, obs = permeabilityTest)

#### Mars Model with no preprocessing

\# Create the tune grid

tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_q4.rds")) {

    mars_model <- readRDS("models/mars_model_q4.rds")

} else {

    mars_model <- train(x = fingerprintsTrain, y = permeabilityTrain, trControl = ctrl, tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_q4.rds")

}

\# Print the model

mars_model

\# Plot the model

plot(mars_model)

\# Make the prediction 

mars_pred <- predict(mars_model, newdata = fingerprintsTest)

\# Get the performance scores

postResample(pred = mars_pred, obs = permeabilityTest)

#### Mars Model removing correlation

\# Remove the highly correlated value

highlyCorDescr <- findCorrelation(fingerprints_filtered_df, cutoff = .75)

fingerprints_filtered_cor_df <- fingerprints_filtered_df[,-highlyCorDescr]

\# Split the data

fingerprintsCorTrain <- fingerprints_filtered_cor_df[cv_index,]

fingerprintsCorTest <- fingerprints_filtered_cor_df[-cv_index,]

\# Create the tune grid

tune_grid <- expand.grid(.degree = 1:3, .nprune = 2:38)

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/mars_model_cor_q4.rds")) {

    mars_model <- readRDS("models/mars_model_cor_q4.rds")

} else {

    mars_model <- train(x = fingerprintsCorTrain, y = permeabilityTrain, trControl = ctrl, preProcess = "spatialSign", tuneGrid = tune_grid, method = "earth")

    saveRDS(mars_model, "models/mars_model_cor_q4.rds")

}

\# Print the model

mars_model

\# Plot the model

plot(mars_model)

\# Make the prediction 

mars_pred <- predict(mars_model, newdata = fingerprintsCorTest)

\# Get the performance scores

postResample(pred = mars_pred, obs = permeabilityTest)

#### Support Vector Machine

\# Setting the seed

set.seed(1)

\# Check the file exists and load to variables

\# else bulid and store the model

if(file.exists("models/svm_model_q4.rds")) {

    svm_model <- readRDS("models/svm_model_q4.rds")

} else {

    svm_model <- train(x = fingerprintsTrain, y = permeabilityTrain, tuneLength = 14, method = "svmRadial", 

                       trControl = ctrl, preProc = c("center","scale"))

    saveRDS(svm_model, "models/svm_model_q4.rds")

}

\# Print the model

svm_model

\# Plot the model

plot(svm_model)

\# Make the prediction 

svm_pred <- predict(svm_model, newdata = fingerprintsTest)

\# Get the performance scores

postResample(pred = svm_pred, obs = permeabilityTest)

#### End of the Assignemnt